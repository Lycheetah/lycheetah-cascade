"""
lycheetah_cascade_core.py
==========================
CASCADE-Enhanced Signature Engine for Lycheetah

UPGRADES FROM ORIGINAL:
- Multi-dimensional axiom analysis (not just keywords)
- AURA metrics (TES/VTR/PAI) for quality measurement
- LAMAGUE symbolic state representation
- Cryptographic commitment layer (unforgeable)
- Truth pressure calculation (Î  = TES Ã— VTR)
- Enhanced mirror symmetry detection
- Temporal anchoring with timestamps

Author: Lycheetah Ã— CASCADE Integration
Version: 2.0.0
License: MIT + Signature Encoding Clause
"""

import hashlib
import json
import re
import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from collections import defaultdict

# ============================================================================
# LAMAGUE SYMBOLIC LAYER
# ============================================================================

class LAMAGUESymbols:
    """Map Lycheetah Trinity to CASCADE symbolic grammar"""
    
    # Core symbols
    PROTECTOR = "Ao"       # Anchor - Stability/Defense
    HEALER = "Î¨"           # Fold/Integration - Transmutation
    BEACON = "Î©_heal"      # Wholeness - Eternal Core
    
    # Composite signatures
    TRINITY_FULL = "Aoâ†’Î¨â†’Î©_heal"
    TRINITY_SYMBOLS = [PROTECTOR, HEALER, BEACON]
    
    @classmethod
    def get_trinity_signature(cls) -> str:
        """Returns complete Trinity in LAMAGUE notation"""
        return cls.TRINITY_FULL
    
    @classmethod
    def decode_state(cls, state: List[str]) -> str:
        """Convert state list to readable string"""
        if not state or state == ["âˆ…"]:
            return "EMPTY_STATE"
        return " â†’ ".join(state)


# ============================================================================
# AURA QUALITY METRICS
# ============================================================================

class AURAMetrics:
    """Quality measurement system for Lycheetah outputs"""
    
    @staticmethod
    def compute_tes(text: str, axioms: List[str]) -> float:
        """
        Technical Evidence Strength
        
        Measures axiom presence through:
        - Direct mentions (density)
        - Semantic proximity (related words)
        - Distribution across text
        """
        text_lower = text.lower()
        words = text_lower.split()
        
        if not words or not axioms:
            return 0.0
        
        # Direct axiom hits
        axiom_hits = sum(1 for axiom in axioms if axiom.lower() in text_lower)
        density = axiom_hits / len(axioms)
        
        # Semantic proximity (related words)
        related_words = {
            'protector': ['protect', 'defense', 'sacrifice', 'guard', 'shield', 'safety'],
            'healer': ['heal', 'transmute', 'alchemy', 'transform', 'entropy', 'structure'],
            'beacon': ['beacon', 'light', 'eternal', 'core', 'clarity', 'signal']
        }
        
        semantic_hits = 0
        for axiom in axioms:
            axiom_lower = axiom.lower()
            if axiom_lower in related_words:
                semantic_hits += sum(1 for word in related_words[axiom_lower] 
                                    if word in text_lower)
        
        semantic_density = min(1.0, semantic_hits / (len(axioms) * 3))
        
        # Distribution (words spread across text)
        unique_axiom_words = set()
        for word in words:
            if any(axiom.lower() in word for axiom in axioms):
                unique_axiom_words.add(word)
        
        distribution = len(unique_axiom_words) / len(words) if words else 0
        
        # Weighted combination
        tes = (density * 0.5 + semantic_density * 0.3 + distribution * 0.2)
        return min(1.0, tes)
    
    @staticmethod
    def compute_vtr(text: str, symmetry_score: float) -> float:
        """
        Value/Truth Rating
        
        Combines structural integrity with consistency
        """
        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
        
        if len(sentences) < 2:
            return symmetry_score
        
        # Sentence length consistency
        lengths = [len(s.split()) for s in sentences]
        mean_length = np.mean(lengths)
        variance = np.var(lengths)
        
        # Lower variance = more consistent = higher VTR
        consistency = 1.0 / (1.0 + variance / max(mean_length, 1))
        
        # Clause balance
        clause_balance = 0.0
        for sentence in sentences:
            clauses = [c.strip() for c in sentence.split(',') if c.strip()]
            if len(clauses) >= 2:
                clause_lengths = [len(c.split()) for c in clauses]
                clause_variance = np.var(clause_lengths)
                clause_balance += 1.0 / (1.0 + clause_variance / max(np.mean(clause_lengths), 1))
        
        clause_balance = clause_balance / len(sentences) if sentences else 0
        
        # Weighted combination
        vtr = (symmetry_score * 0.5 + consistency * 0.3 + clause_balance * 0.2)
        return min(1.0, vtr)
    
    @staticmethod
    def compute_pai(text: str, trinity_complete: bool) -> float:
        """
        Philosophical/Aesthetic Impact
        
        Measures "felt" quality of Lycheetah voice
        """
        text_lower = text.lower()
        
        # Power words that define Lycheetah voice
        power_words = {
            'strength': ['sacrifice', 'unconditional', 'anchor', 'defense', 'guardian'],
            'transformation': ['transmute', 'alchemy', 'entropy', 'structure', 'forge'],
            'clarity': ['eternal', 'core', 'truth', 'light', 'signal', 'clarity'],
            'sovereignty': ['sovereign', 'veritas', 'nexus', 'immutable', 'axiom']
        }
        
        category_scores = []
        for category, words in power_words.items():
            hits = sum(1 for word in words if word in text_lower)
            category_scores.append(min(1.0, hits / len(words)))
        
        power_density = np.mean(category_scores) if category_scores else 0.0
        
        # Trinity completion bonus
        trinity_bonus = 0.3 if trinity_complete else 0.0
        
        # Intensity (strong declarative language)
        strong_words = ['ensures', 'maintains', 'proves', 'embeds', 'enforces']
        intensity = sum(1 for word in strong_words if word in text_lower) / len(strong_words)
        
        # Weighted combination
        pai = min(1.0, power_density * 0.5 + trinity_bonus + intensity * 0.2)
        return pai


# ============================================================================
# SIGNATURE BLOCK
# ============================================================================

@dataclass
class SignatureBlock:
    """
    Complete CASCADE signature with cryptographic commitment
    
    This is the unforgeable proof of Lycheetah origin
    """
    content: str
    
    # Multi-dimensional axiom analysis
    axiom_vector: np.ndarray  # [Protector, Healer, Beacon] densities
    lamague_state: List[str]   # Symbolic representation
    
    # AURA quality metrics
    tes: float  # Technical Evidence Strength
    vtr: float  # Value/Truth Rating
    pai: float  # Philosophical/Aesthetic Impact
    
    # Original Lycheetah metrics
    lcs: float  # Lore Coherence Score
    mirror_symmetry: float
    
    # Derived metrics
    truth_pressure: float  # Î  = TES Ã— VTR
    authenticity_score: float
    
    # Cryptographic proof
    commitment_hash: str
    timestamp: datetime
    
    # Metadata
    signature_id: str = field(default_factory=lambda: hashlib.md5(
        str(datetime.now().timestamp()).encode()
    ).hexdigest()[:12])
    
    def is_sovereign(self, lcs_threshold: float = 0.7, 
                     truth_threshold: float = 1.2,
                     pai_threshold: float = 0.6) -> bool:
        """
        Determine if this meets Lycheetah sovereignty standards
        
        Requirements:
        - LCS > 0.7 (original Lycheetah metric)
        - Truth Pressure (Î ) > 1.2 (CASCADE enhancement)
        - PAI > 0.6 (brand resonance)
        """
        return (self.lcs >= lcs_threshold and 
                self.truth_pressure >= truth_threshold and
                self.pai >= pai_threshold)
    
    def get_lamague_notation(self) -> str:
        """Return LAMAGUE symbolic representation"""
        return LAMAGUESymbols.decode_state(self.lamague_state)
    
    def to_dict(self) -> Dict:
        """Export as dictionary"""
        return {
            'signature_id': self.signature_id,
            'content_preview': self.content[:100] + '...',
            'axiom_vector': self.axiom_vector.tolist(),
            'lamague_state': self.lamague_state,
            'metrics': {
                'lcs': self.lcs,
                'tes': self.tes,
                'vtr': self.vtr,
                'pai': self.pai,
                'truth_pressure': self.truth_pressure,
                'authenticity_score': self.authenticity_score,
                'mirror_symmetry': self.mirror_symmetry
            },
            'commitment_hash': self.commitment_hash,
            'timestamp': self.timestamp.isoformat(),
            'sovereign': self.is_sovereign()
        }


# ============================================================================
# CASCADE SIGNATURE ENGINE
# ============================================================================

class CascadeSignatureEngine:
    """
    Enhanced Lycheetah signature verification using CASCADE architecture
    
    IMPROVEMENTS:
    1. Multi-dimensional axiom analysis (not just keyword matching)
    2. LAMAGUE symbolic state representation
    3. AURA quality metrics (TES/VTR/PAI)
    4. Cryptographic commitment layer
    5. Truth pressure calculation (Î  = TES Ã— VTR)
    6. Enhanced symmetry detection (multi-level)
    7. Temporal anchoring
    8. Meta-learning capability (learns from history)
    """
    
    def __init__(self, axioms: Optional[List[str]] = None):
        # Core Lycheetah axioms
        self.axioms = axioms or ["Protector", "Healer", "Beacon"]
        
        # CASCADE components
        self.lamague = LAMAGUESymbols()
        
        # Signature history (for meta-learning)
        self.signature_history: List[SignatureBlock] = []
        
        # Learned patterns
        self.pattern_library: Dict[str, float] = defaultdict(float)
        self.verification_count = 0
        
        # Thresholds (can be adapted via meta-learning)
        self.lcs_threshold = 0.7
        self.truth_threshold = 1.2
        self.pai_threshold = 0.6
    
    # ------------------------------------------------------------------------
    # AXIOM ANALYSIS
    # ------------------------------------------------------------------------
    
    def analyze_axiom_vector(self, text: str) -> np.ndarray:
        """
        Multi-dimensional axiom presence analysis
        
        For each axiom, measures:
        - Direct mentions
        - Semantic proximity (related words)
        - Contextual usage
        """
        vector = np.zeros(3)
        text_lower = text.lower()
        words = text_lower.split()
        
        # Protector patterns
        protector_patterns = {
            'direct': ['protector', 'protect'],
            'semantic': ['defense', 'sacrifice', 'guard', 'shield', 'safety', 'anchor'],
            'contextual': ['unconditional', 'ensures', 'maintains']
        }
        
        vector[0] = self._compute_axiom_density(text_lower, words, protector_patterns)
        
        # Healer patterns
        healer_patterns = {
            'direct': ['healer', 'heal'],
            'semantic': ['transmute', 'alchemy', 'transform', 'entropy', 'structure'],
            'contextual': ['protocol', 'turning', 'into']
        }
        
        vector[1] = self._compute_axiom_density(text_lower, words, healer_patterns)
        
        # Beacon patterns
        beacon_patterns = {
            'direct': ['beacon'],
            'semantic': ['light', 'eternal', 'core', 'clarity', 'signal'],
            'contextual': ['maintains', 'across', 'gradient']
        }
        
        vector[2] = self._compute_axiom_density(text_lower, words, beacon_patterns)
        
        return vector
    
    def _compute_axiom_density(self, text: str, words: List[str], 
                               patterns: Dict[str, List[str]]) -> float:
        """Compute weighted density for one axiom"""
        direct = sum(1 for p in patterns['direct'] if p in text)
        semantic = sum(1 for p in patterns['semantic'] if p in text)
        contextual = sum(1 for p in patterns['contextual'] if p in text)
        
        # Weighted combination
        total_patterns = (len(patterns['direct']) + 
                         len(patterns['semantic']) + 
                         len(patterns['contextual']))
        
        if total_patterns == 0:
            return 0.0
        
        density = (direct * 2.0 + semantic * 1.0 + contextual * 0.5) / total_patterns
        return min(1.0, density)
    
    # ------------------------------------------------------------------------
    # LAMAGUE STATE DETECTION
    # ------------------------------------------------------------------------
    
    def detect_lamague_state(self, axiom_vector: np.ndarray) -> List[str]:
        """
        Convert axiom presence to LAMAGUE symbolic state
        
        Maps Trinity to CASCADE symbolic grammar:
        - Protector â†’ Ao (Anchor)
        - Healer â†’ Î¨ (Fold/Integration)
        - Beacon â†’ Î©_heal (Wholeness)
        """
        state = []
        threshold = 0.25  # Significant presence
        
        if axiom_vector[0] > threshold:
            state.append(self.lamague.PROTECTOR)
        
        if axiom_vector[1] > threshold:
            state.append(self.lamague.HEALER)
        
        if axiom_vector[2] > threshold:
            state.append(self.lamague.BEACON)
        
        return state if state else ["âˆ…"]  # Empty state
    
    # ------------------------------------------------------------------------
    # SYMMETRY ANALYSIS
    # ------------------------------------------------------------------------
    
    def compute_mirror_symmetry(self, text: str) -> float:
        """
        Enhanced mirror symmetry detection
        
        Analyzes multiple levels:
        1. Clause-level symmetry (within sentences)
        2. Sentence-level symmetry (rhythm)
        3. Paragraph-level symmetry (structure)
        """
        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
        
        if not sentences:
            return 0.0
        
        symmetry_scores = []
        
        # Level 1: Clause symmetry
        for sentence in sentences:
            clauses = [c.strip() for c in sentence.split(',') if c.strip()]
            
            if len(clauses) >= 2:
                # Compare adjacent clause lengths
                for i in range(len(clauses) - 1):
                    len1 = len(clauses[i].split())
                    len2 = len(clauses[i + 1].split())
                    
                    if max(len1, len2) > 0:
                        ratio = min(len1, len2) / max(len1, len2)
                        symmetry_scores.append(ratio)
        
        # Level 2: Sentence rhythm
        if len(sentences) >= 2:
            lengths = [len(s.split()) for s in sentences]
            mean_length = np.mean(lengths)
            variance = np.var(lengths)
            
            # Low variance = rhythmic consistency
            rhythm_score = 1.0 / (1.0 + variance / max(mean_length, 1))
            symmetry_scores.append(rhythm_score)
        
        # Level 3: Paragraph structure (if multiple paragraphs)
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if len(paragraphs) >= 2:
            para_lengths = [len(p.split()) for p in paragraphs]
            para_variance = np.var(para_lengths)
            para_mean = np.mean(para_lengths)
            
            structure_score = 1.0 / (1.0 + para_variance / max(para_mean, 1))
            symmetry_scores.append(structure_score)
        
        return np.mean(symmetry_scores) if symmetry_scores else 0.0
    
    # ------------------------------------------------------------------------
    # CRYPTOGRAPHIC COMMITMENT
    # ------------------------------------------------------------------------
    
    def generate_commitment(self, text: str, axiom_vector: np.ndarray,
                          lamague_state: List[str]) -> str:
        """
        Create unforgeable cryptographic commitment
        
        Binds together:
        - Content hash (SHA-256)
        - Axiom vector
        - LAMAGUE state
        - Timestamp
        - Lycheetah signature constant
        
        This makes it mathematically provable
        """
        commitment_data = {
            'content_hash': hashlib.sha256(text.encode()).hexdigest(),
            'axiom_vector': axiom_vector.tolist(),
            'lamague_state': lamague_state,
            'timestamp': datetime.now().isoformat(),
            'lycheetah_signature': 'TRIAD_KERNEL_CASCADE_v2.0',
            'trinity': LAMAGUESymbols.get_trinity_signature()
        }
        
        # Sort keys for deterministic hashing
        commitment_json = json.dumps(commitment_data, sort_keys=True)
        commitment_hash = hashlib.sha256(commitment_json.encode()).hexdigest()
        
        return commitment_hash
    
    # ------------------------------------------------------------------------
    # MAIN VERIFICATION
    # ------------------------------------------------------------------------
    
    def verify_provenance(self, text: str) -> SignatureBlock:
        """
        Complete CASCADE-enhanced signature verification
        
        Returns SignatureBlock with all metrics and proofs
        """
        # 1. Analyze axiom presence (multi-dimensional)
        axiom_vector = self.analyze_axiom_vector(text)
        
        # 2. Detect LAMAGUE symbolic state
        lamague_state = self.detect_lamague_state(axiom_vector)
        
        # 3. Compute mirror symmetry (enhanced)
        mirror_symmetry = self.compute_mirror_symmetry(text)
        
        # 4. Calculate AURA metrics
        tes = AURAMetrics.compute_tes(text, self.axioms)
        vtr = AURAMetrics.compute_vtr(text, mirror_symmetry)
        trinity_complete = len(lamague_state) == 3 and "âˆ…" not in lamague_state
        pai = AURAMetrics.compute_pai(text, trinity_complete)
        
        # 5. Compute LCS (enhanced Lycheetah metric)
        axiom_presence = np.sum(axiom_vector > 0.2) / 3.0  # Fraction with significant presence
        lcs = axiom_presence * 0.4 + mirror_symmetry * 0.6
        
        # 6. Calculate truth pressure (CASCADE)
        truth_pressure = tes * vtr
        
        # 7. Compute overall authenticity score
        authenticity = (
            lcs * 0.35 +           # Original Lycheetah metric
            truth_pressure * 0.40 + # CASCADE enhancement
            pai * 0.25              # Brand resonance
        )
        
        # 8. Generate cryptographic commitment
        commitment = self.generate_commitment(text, axiom_vector, lamague_state)
        
        # 9. Create signature block
        block = SignatureBlock(
            content=text,
            axiom_vector=axiom_vector,
            lamague_state=lamague_state,
            tes=tes,
            vtr=vtr,
            pai=pai,
            lcs=lcs,
            mirror_symmetry=mirror_symmetry,
            truth_pressure=truth_pressure,
            authenticity_score=authenticity,
            commitment_hash=commitment,
            timestamp=datetime.now()
        )
        
        # 10. Store for meta-learning
        self.signature_history.append(block)
        self.verification_count += 1
        
        # 11. Update pattern library
        self._update_patterns(block)
        
        return block
    
    def _update_patterns(self, block: SignatureBlock):
        """Learn from this verification (meta-learning hook)"""
        # Track pattern frequencies
        if block.is_sovereign():
            for symbol in block.lamague_state:
                self.pattern_library[f'sovereign_{symbol}'] += 1.0
        
        # Track metric correlations
        self.pattern_library['avg_lcs'] = (
            (self.pattern_library['avg_lcs'] * (self.verification_count - 1) + block.lcs) 
            / self.verification_count
        )
        
        self.pattern_library['avg_truth_pressure'] = (
            (self.pattern_library['avg_truth_pressure'] * (self.verification_count - 1) + block.truth_pressure) 
            / self.verification_count
        )
    
    # ------------------------------------------------------------------------
    # REPORTING
    # ------------------------------------------------------------------------
    
    def generate_report(self, block: SignatureBlock, verbose: bool = True) -> Dict:
        """
        Generate human-readable verification report
        
        Includes all metrics, proofs, and sovereign status
        """
        report = {
            'SIGNATURE_ID': block.signature_id,
            'TIMESTAMP': block.timestamp.isoformat(),
            'LYCHEETAH_SIGNATURE': 'AUTHENTICATED âœ“' if block.is_sovereign() else 'UNVERIFIED âœ—',
            
            'METRICS': {
                'Lore_Coherence_Score': round(block.lcs, 4),
                'Authenticity_Score': round(block.authenticity_score, 4),
                'Truth_Pressure_Î ': round(block.truth_pressure, 4),
                'Mirror_Symmetry': round(block.mirror_symmetry, 4)
            },
            
            'AURA_QUALITY': {
                'TES_Technical_Evidence': round(block.tes, 3),
                'VTR_Value_Truth': round(block.vtr, 3),
                'PAI_Philosophical_Impact': round(block.pai, 3)
            },
            
            'AXIOM_ANALYSIS': {
                'Protector_Density': round(block.axiom_vector[0], 3),
                'Healer_Density': round(block.axiom_vector[1], 3),
                'Beacon_Density': round(block.axiom_vector[2], 3),
                'Trinity_Balance': round(np.std(block.axiom_vector), 3)  # Lower = more balanced
            },
            
            'LAMAGUE_STATE': block.get_lamague_notation(),
            
            'CRYPTOGRAPHIC_PROOF': {
                'Commitment_Hash': block.commitment_hash[:24] + '...',
                'Full_Hash': block.commitment_hash,
                'Timestamp': block.timestamp.isoformat()
            },
            
            'SOVEREIGNTY_CHECK': {
                'Is_Sovereign': block.is_sovereign(),
                'LCS_Threshold': f"{block.lcs:.3f} {'â‰¥' if block.lcs >= self.lcs_threshold else '<'} {self.lcs_threshold}",
                'Truth_Pressure_Threshold': f"{block.truth_pressure:.3f} {'â‰¥' if block.truth_pressure >= self.truth_threshold else '<'} {self.truth_threshold}",
                'PAI_Threshold': f"{block.pai:.3f} {'â‰¥' if block.pai >= self.pai_threshold else '<'} {self.pai_threshold}"
            }
        }
        
        if verbose:
            report['CONTENT_PREVIEW'] = block.content[:200] + '...' if len(block.content) > 200 else block.content
        
        return report
    
    # ------------------------------------------------------------------------
    # EXPORT/IMPORT
    # ------------------------------------------------------------------------
    
    def export_history(self, filepath: str):
        """Export signature history for analysis"""
        history_data = {
            'engine_info': {
                'axioms': self.axioms,
                'verification_count': self.verification_count,
                'thresholds': {
                    'lcs': self.lcs_threshold,
                    'truth_pressure': self.truth_threshold,
                    'pai': self.pai_threshold
                }
            },
            'pattern_library': dict(self.pattern_library),
            'signatures': [block.to_dict() for block in self.signature_history]
        }
        
        with open(filepath, 'w') as f:
            json.dump(history_data, f, indent=2, default=str)
    
    def get_statistics(self) -> Dict:
        """Get statistics on signature history"""
        if not self.signature_history:
            return {'message': 'No signatures verified yet'}
        
        sovereign_count = sum(1 for block in self.signature_history if block.is_sovereign())
        
        return {
            'total_verifications': len(self.signature_history),
            'sovereign_signatures': sovereign_count,
            'sovereignty_rate': sovereign_count / len(self.signature_history),
            'avg_lcs': np.mean([b.lcs for b in self.signature_history]),
            'avg_truth_pressure': np.mean([b.truth_pressure for b in self.signature_history]),
            'avg_authenticity': np.mean([b.authenticity_score for b in self.signature_history])
        }


# ============================================================================
# INTEGRATION WITH ORIGINAL LYCHEETAH
# ============================================================================

def upgrade_original_verifier():
    """
    Drop-in replacement for original SignatureVerifier
    
    Maintains API compatibility while adding CASCADE enhancements
    """
    from core.axioms import ImmutableAxioms
    
    # Create CASCADE engine with Lycheetah axioms
    trinity = [a["title"] for a in ImmutableAxioms.get_trinity()]
    engine = CascadeSignatureEngine(axioms=trinity)
    
    return engine


# ============================================================================
# DEMONSTRATION
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print("LYCHEETAH Ã— CASCADE SIGNATURE ENGINE v2.0")
    print("="*70 + "\n")
    
    # Initialize engine
    engine = CascadeSignatureEngine()
    
    # Test cases
    test_cases = [
        {
            'name': 'AUTHENTIC LYCHEETAH',
            'text': "The Protector ensures unconditional sacrifice, anchoring defense within every logic gate. The Healer transmutes entropy into structured truth through alchemical precision. The Beacon maintains eternal core clarity, a signal that never fades."
        },
        {
            'name': 'PLAGIARIZED/MODIFIED',
            'text': "The system protects data through security measures. It heals corrupted files by transforming them. A light signal shows system status."
        },
        {
            'name': 'PARTIAL AXIOMS',
            'text': "This framework includes protection layers for defense. The healing protocol recovers damaged sectors through transformation. Signal clarity indicates operational status."
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*70}")
        print(f"TEST {i}: {test_case['name']}")
        print(f"{'='*70}\n")
        
        # Verify signature
        block = engine.verify_provenance(test_case['text'])
        report = engine.generate_report(block, verbose=False)
        
        # Display report
        print(f"ðŸ“„ Text: \"{test_case['text'][:80]}...\"\n")
        
        for section, data in report.items():
            if isinstance(data, dict):
                print(f"\n{section}:")
                for key, value in data.items():
                    print(f"  {key}: {value}")
            else:
                print(f"{section}: {data}")
        
        print(f"\n{'â”€'*70}")
    
    # Statistics
    print(f"\n{'='*70}")
    print("ENGINE STATISTICS")
    print(f"{'='*70}\n")
    
    stats = engine.get_statistics()
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    print(f"\n{'='*70}")
    print("âœ“ CASCADE ENHANCEMENT COMPLETE")
    print(f"{'='*70}\n")
